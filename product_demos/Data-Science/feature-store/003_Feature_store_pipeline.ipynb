{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "8eeeb251-5f27-4f06-99db-734f1c3b2371",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Create feature tables in UC with Spark Declarative Pipelines\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/feature_store/2025Q4_03_image1.png?raw=true\" style=\"float: right\" width=\"500px\" />\n",
    "\n",
    "Any streaming table or materialized view in Unity Catalog with a primary key can be a feature table in Unity Catalog, and you can use the Features UI and API with the table.\n",
    "\n",
    "In order to run the notebook, please go to the pipeline UI to create a pipeline.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f68e88a-c4b2-441a-baca-10f263d7b5a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql import functions as F, Window as W\n",
    "from pyspark import pipelines as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48aa3313-49ea-4b08-9a78-5c95e876a167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dp.materialized_view(\n",
    "    name=\"travel_purchase_sdp\",\n",
    "    schema=\"\"\"\n",
    "        destination_id BIGINT NOT NULL,\n",
    "        user_id BIGINT NOT NULL,\n",
    "        ts TIMESTAMP,\n",
    "        clicked BOOLEAN,\n",
    "        purchased BOOLEAN,\n",
    "        price DOUBLE\n",
    "    \"\"\"\n",
    ")\n",
    "def travel_purchase_sdp():\n",
    "    df = (\n",
    "        spark.table(\"travel_purchase\")\n",
    "        .selectExpr(\n",
    "            \"CAST(destination_id AS BIGINT) AS destination_id\",\n",
    "            \"CAST(user_id AS BIGINT) AS user_id\",\n",
    "            \"ts\",\n",
    "            \"clicked\",\n",
    "            \"purchased\",\n",
    "            \"price\"\n",
    "        )\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "997edcc9-d313-451c-b032-52cb53e03a31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dp.materialized_view(\n",
    "    name=\"user_demography_sdp\",\n",
    "    schema=\"\"\"\n",
    "        user_id BIGINT NOT NULL,\n",
    "        age BIGINT,\n",
    "        gender STRING,\n",
    "        income_bracket STRING,\n",
    "        loyalty_tier STRING,\n",
    "        first_login_date TIMESTAMP,\n",
    "        billing_state STRING,\n",
    "        billing_city STRING\n",
    "    \"\"\"\n",
    ")\n",
    "def user_demography_sdp():\n",
    "    df = (\n",
    "        spark.table(\"user_demography\")\n",
    "        .selectExpr(\n",
    "            \"CAST(user_id AS BIGINT) AS user_id\",\n",
    "            \"CAST(age AS BIGINT) AS age\",\n",
    "            \"gender\",\n",
    "            \"income_bracket\",\n",
    "            \"loyalty_tier\",\n",
    "            \"first_login_date\",\n",
    "            \"billing_state\",\n",
    "            \"billing_city\"\n",
    "        )\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2f1927f-6994-4ccb-a226-fb8f7b976d95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dp.materialized_view(\n",
    "    name=\"destination_features_advanced_sdp\",\n",
    "    schema=\"\"\"\n",
    "        destination_id BIGINT NOT NULL,\n",
    "        ts TIMESTAMP,\n",
    "        sum_clicks_7d DOUBLE,\n",
    "        sum_impressions_7d DOUBLE,\n",
    "        CONSTRAINT destination_features_advanced_pk_sdp PRIMARY KEY (destination_id)\n",
    "    \"\"\"\n",
    ")\n",
    "def destination_features_advanced():\n",
    "    df = spark.table(\"travel_purchase_sdp\").select(\"destination_id\", \"ts\", \"clicked\")\n",
    "\n",
    "    window_spec = W.partitionBy(\"destination_id\").orderBy(F.col(\"ts\").cast(\"long\")).rangeBetween(-7*86400, 0)\n",
    "\n",
    "    df = (\n",
    "        df.withColumn(\"clicked_i\", F.col(\"clicked\").cast(\"int\"))\n",
    "          .withColumn(\"sum_clicks_7d\", F.sum(\"clicked_i\").over(window_spec).cast(\"double\"))\n",
    "          .withColumn(\"sum_impressions_7d\", F.count(\"*\").over(window_spec).cast(\"double\"))\n",
    "          # Fix datatype mismatches explicitly\n",
    "          .withColumn(\"destination_id\", F.col(\"destination_id\").cast(\"bigint\"))\n",
    "          .withColumn(\"ts\", F.col(\"ts\").cast(\"timestamp\"))\n",
    "          .select(\"destination_id\", \"ts\", \"sum_clicks_7d\", \"sum_impressions_7d\")\n",
    "    )\n",
    "    # Optional: fill nulls to avoid nullable conflicts\n",
    "    df = df.na.fill({\"sum_clicks_7d\": 0.0, \"sum_impressions_7d\": 0.0})\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deb2ac36-3186-44a9-aba2-8954d43f9011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dp.materialized_view(\n",
    "    name=\"user_features_advanced_sdp\",\n",
    "    schema=\"\"\"\n",
    "        user_id BIGINT NOT NULL,\n",
    "        ts TIMESTAMP,\n",
    "        mean_price_7d DOUBLE,\n",
    "        last_6m_purchases DOUBLE,\n",
    "        tenure_days DOUBLE,\n",
    "        age BIGINT,\n",
    "        gender STRING,\n",
    "        income_bracket STRING,\n",
    "        loyalty_tier STRING,\n",
    "        billing_state STRING,\n",
    "        billing_city STRING,\n",
    "        CONSTRAINT user_features_advanced_pk_sdp PRIMARY KEY (user_id)\n",
    "    \"\"\"\n",
    ")\n",
    "def user_features_advanced():\n",
    "    travel_purchase = spark.table(\"travel_purchase_sdp\").select(\"user_id\", \"price\", \"purchased\", \"ts\")\n",
    "    user_demo = spark.table(\"user_demography_sdp\")\n",
    "\n",
    "    window_spec = W.partitionBy(\"user_id\").orderBy(F.col(\"ts\").cast(\"long\"))\n",
    "\n",
    "    user_feat = (\n",
    "        travel_purchase\n",
    "        .withColumn(\"ts_l\", F.col(\"ts\").cast(\"long\"))\n",
    "        .withColumn(\n",
    "            \"lookedup_price_7d_rolling_sum\",\n",
    "            F.sum(\"price\").over(window_spec.rangeBetween(-7*86400, 0))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"lookups_7d_rolling_sum\",\n",
    "            F.count(\"*\").over(window_spec.rangeBetween(-7*86400, 0))\n",
    "        )\n",
    "        .withColumn(\"mean_price_7d\", (F.col(\"lookedup_price_7d_rolling_sum\") / F.col(\"lookups_7d_rolling_sum\")).cast(\"double\"))\n",
    "        .withColumn(\"tickets_purchased\", F.col(\"purchased\").cast(\"int\"))\n",
    "        .withColumn(\n",
    "            \"last_6m_purchases\",\n",
    "            F.sum(\"tickets_purchased\").over(window_spec.rangeBetween(-6*30*86400, 0)).cast(\"double\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    user_feat = (\n",
    "        user_feat.join(user_demo, on=\"user_id\", how=\"left\")\n",
    "                 .withColumn(\"tenure_days\", F.datediff(F.current_date(), F.col(\"first_login_date\")).cast(\"double\"))\n",
    "                 .withColumn(\"user_id\", F.col(\"user_id\").cast(\"bigint\"))\n",
    "                 .withColumn(\"ts\", F.col(\"ts\").cast(\"timestamp\"))\n",
    "                 .select(\n",
    "                     \"user_id\", \"ts\", \"mean_price_7d\", \"last_6m_purchases\", \"tenure_days\",\n",
    "                     \"age\", \"gender\", \"income_bracket\", \"loyalty_tier\", \"billing_state\", \"billing_city\"\n",
    "                 )\n",
    "    )\n",
    "\n",
    "    # Optional: handle nulls for numeric columns to keep schema strict\n",
    "    user_feat = user_feat.na.fill({\n",
    "        \"mean_price_7d\": 0.0,\n",
    "        \"last_6m_purchases\": 0.0,\n",
    "        \"tenure_days\": 0.0\n",
    "    })\n",
    "\n",
    "    return user_feat\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_0c235d96-4bc7-4fb5-b118-17fd1dad0124",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "003_Feature_store_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
